{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Captcha Recognition Model\n",
    "By Luiz Bossetto -- CS387 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This project is an approach to solving text-based Captcha images using deep learning concepts related to image classification. This notebook contains all the steps taken to tackle this challenge head-on, from data preprocessing to model development, training, and evaluation. This notebook aims to demonstrate the effectiveness of deep learning in overcoming CAPTCHA challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Motivation\n",
    "While working on a paper for a History classe that I am taking this semester, I had a quite frustrating experience accessing an academic paper hosted in a specific website. Every time I refreshed the browser tab, some security system would block my access and make me solve a text-based captcha puzzle. What made this a frustrating experience for me was that I had to go through the same process of solving the puzzle every time, even though I had solved it a minute ago. So I thought to myself: \"Why not build something that can help me with this?\". This is how I came up with my final project idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What topics should you know before diving into the project?\n",
    "* Knowing the following topics will enhance your experience understanding and going through the project:\n",
    "    * Basic knowledge of PyTorch: how to define a custom dataset, how to define a Convolutional Neural Network (CNN).\n",
    "    * Familiarity with deep-learning-related topics such as data normalization, character segmentation, dataset splitting, and the steps it takes to train a model.\n",
    "    * Understanding of binary thresholding, bitwise operations in images, and morphological operations (used in noise reduction) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by step\n",
    "I have decided to approach this challenge by breaking the problem down into smaller steps:\n",
    "\n",
    "1. Setup - All requirements will be installed in the notebook.  \n",
    "2. Data Processing - Given a dataset of captcha images, the model will pass them to a noise reduction filter. This filter will be responsible for removing the noise present in the image.\n",
    "3. Data segmentation - After the images are passed to the nosie reduction filter, the model will crop the characters (\"areas of interest\") and store them in a folder for character classification.\n",
    "4. Data setup - Save all images in a custom dataset class. This can be used using pytorch's tools from <code>torch.utils.data</code>.\n",
    "5. Training - The model wil be trained on the cropped characters from the denoised images provided by the noise reduction filter. This is a character classification problem that will involve 35 possible classes.\n",
    "6. Evalutation - The model will be evaluated using both the training and test set created by a random data split to avoid any possible bias. In this step, I aim to find the best hyperparameters for maximized performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup\n",
    "\n",
    "For this project, a few python modules were necessary:\n",
    "\n",
    "* OpenCV (cv2) -- OpenCV is a library of programming functions mainly for real-time computer vision. This will be used in the data preprocessing and segmentation step.\n",
    "* Numpy -- NumPy is a library that adds support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions. This will be used in the data preprocessing and character segmentation step.\n",
    "* Matplotlib -- Matplotlib is a plotting library. It provides an API for embedding plots into applications. This will be used in the model evaluation step.\n",
    "* Torch -- PyTorch is a machine learning library based on the Torch library, used for applications such as computer vision and natural language processing. This will be used as the main framework throughout the whole project.\n",
    "* PIL (Python Imaging Library) -- PIL is a library for that adds support for opening, manipulating, and saving many different image file formats.\n",
    "* os -- The OS module provides a portable way of using operating system dependent functionality. In particular, this will be used in the dataprocessing step to properly store data in the right folders. <br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python in ./.local/lib/python3.10/site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in ./.local/lib/python3.10/site-packages (from opencv-python) (1.26.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (1.26.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in ./.local/lib/python3.10/site-packages (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.local/lib/python3.10/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.local/lib/python3.10/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.local/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.21 in ./.local/lib/python3.10/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.10/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/lib/python3/dist-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.local/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install numpy\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset, random_split\n",
    "from torch import optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # plotting data after evaluation\n",
    "import os # used for path and image storage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, using the GPU is better than the CPU due to parallelism: a device's ability to run several calculations simultaneously. By using the GPU, I could train the model using several examples at a time. This makes the process more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Set device to point to a GPU if we have one, CPU otherwise.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preprocessing\n",
    "\n",
    "The first part of the model involves preprocessing the images in the dataset. This is an important step because this task involves noisy images, and the easier it is for the model to deal with it, the better.\n",
    "\n",
    "This step contains important methods for data preprocessing. They include: \n",
    "\n",
    "### <code>remove_noise(image_path, save_path)</code> \n",
    "This is the function that will be responsible for removing the noise in the captcha images using functionalities from the cv2 module. <br>\n",
    "\n",
    "#### How it works\n",
    "1. The filter first converts the image to grayscale to make it easier to deal with the colors.\n",
    "2. Turns the new image into a numpy array to apply binary thresholding (binarization).\n",
    "3. Apllies binary thresholding that will turn the image into a black-and-white image.\n",
    "4. Apply morphological operations using a 3x3 kernel to remove the noise from the image.\n",
    "5. Turns this black-and-white image (with gray pixels) into a pure black and white image through bitwise operations. This will normalize the data by keeping the range of its pixels in between 0 and 1 instead of 0 and 255.\n",
    "6. After this process, the new image is stored in a folder.\n",
    "\n",
    "It takes two parameters: <br>\n",
    "* <code>image_path</code>: where the image dataset is located (folder).\n",
    "* <code>save_path</code>: where the new dataset will be stored (folder).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(image_path, save_path):\n",
    "    # Open the image using Pillow\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray_image = image.convert('L')\n",
    "\n",
    "    # Convert PIL image to numpy array\n",
    "    np_image = np.array(gray_image)\n",
    "\n",
    "    # Apply binary thresholding\n",
    "    _, binary_image = cv2.threshold(np_image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Apply morphological operations\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    opening = cv2.morphologyEx(binary_image, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "\n",
    "    # Invert the binary image\n",
    "    inverted_image = cv2.bitwise_not(opening)\n",
    "\n",
    "    # Save the inverted binary image\n",
    "    inverted_image_pil = Image.fromarray(inverted_image)\n",
    "\n",
    "    # Invert the colors again to have black background and white letters\n",
    "    inverted_image_pil = inverted_image_pil.convert('L')\n",
    "    inverted_image = np.array(inverted_image_pil)\n",
    "    inverted_image = cv2.bitwise_not(inverted_image)\n",
    "\n",
    "    # Save the final image in output directory\n",
    "    final_image = Image.fromarray(inverted_image)\n",
    "    final_image.save(save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Segmentation\n",
    "This step involves image segmentation. The images generated by the <code>remove_noise()</code> method will be transferred to a method that will be responsible for identifying the areas of interest, or characters.\n",
    "<br>\n",
    "\n",
    "### <code>resize_image(image, new_width, new_height)</code> \n",
    "This is the function that will be responsible for resizing the cropped characters from the denoised images generated by the <code>remove_noise()</code> function. This step is considered part of the data normalization process by resizing all images that will be transferred to the network to a standard size. <br>\n",
    "\n",
    "#### How it works\n",
    "1. It takes images of different shapes and resizes them to a new <code>new_width</code> x <code>new_height</code> image.\n",
    "2. Applies binary thresholding to ensure black and white pixels only. While working on this method, it could be seen that the resized image would not be pure black and white, so binary thresholding was added to prevent this.\n",
    "3. Returns a resized image. \n",
    "\n",
    "It takes two parameters: <br>\n",
    "* <code>image_path</code>: where the denoised images are located (folder).\n",
    "* <code>save_path</code>: where the cropped characters will be stored (folder).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize the image\n",
    "def resize_image(image, new_width, new_height):\n",
    "    # Resize the image\n",
    "    resized_image = cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "    # Apply binary thresholding to ensure black and white pixels only\n",
    "    _, binary_image = cv2.threshold(resized_image, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Return the final inverted image\n",
    "    return binary_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### <code>save_contours_as_images(image_path, output_directory, image_id)</code> \n",
    "This is the function that will be responsible for the selection of areas of interest in a given denoised image generated by the <code>remove_noise()</code> function.\n",
    "\n",
    "#### How it works\n",
    "1. It takes a denoised image and crops contours that are identified by the <code>cv2.findcontours()</code> function.\n",
    "2. Since all portions of white pixels in the image are considered contours in this <code>cv2</code> method, the function should classify which ones are characters and which ones are just noise. This is done by checking the dimensions of the contours. Since all noise follows a similar dimensional pattern, (5x5 pixels), an <code>if</code> statement checking the width and height of each contour is enough to filter the characters from the contours.\n",
    "3. Renames the image to its label, for later label organization.\n",
    "\n",
    "It takes three parameters: <br>\n",
    "* <code>image_path</code>: where the images are located (folder).\n",
    "* <code>output_directory</code>: where the new dataset will be stored (folder).\n",
    "* <code>image_id</code>: an ID to the new generated character image. This is important to prevent file name duplicates. In case images have the the same name, the newly generated one will replace the one that already exists in the folder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_contours_as_images(image_path, output_directory, image_id):\n",
    "    # Load the image in grayscale\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Threshold the image to obtain binary image\n",
    "    _, binary_image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours in the binary image\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Sort contours based on x-coordinate.\n",
    "    # The reason for this is so the images can be correctly labeled to prevent mislabeled data in the future.\n",
    "    contours = sorted(contours, key=lambda contour: cv2.boundingRect(contour)[0])\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    counter = 0 # keep track of how many characters have been saved\n",
    "    \n",
    "    # extract label from image file's name\n",
    "    label = image_path.split('/')[0].split('.')[0].split(\"\\\\\")[1]\n",
    "    image_name = label.split(\"--\")[0]\n",
    "    char_labels = [char_label for char_label in label.split(\"_\")[0]]\n",
    "    \n",
    "    for i, contour in enumerate(contours):\n",
    "        # Get bounding box for each contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "        # if 4 characters have been extracted, break.\n",
    "        if counter > 3:\n",
    "            break\n",
    "\n",
    "        # Check if contour is too small (possibly noise)\n",
    "        if w > 5 and h > 5:\n",
    "            # Add some padding around the character bounding box\n",
    "            padding = 10\n",
    "            x_padding = max(0, x - padding)\n",
    "            y_padding = max(0, y - padding)\n",
    "            w_padding = min(image.shape[1], w + 2 * padding)\n",
    "            h_padding = min(image.shape[0], h + 2 * padding)\n",
    "\n",
    "            # Create a black canvas with padded dimensions\n",
    "            padded_image = np.zeros((h_padding, w_padding), dtype=np.uint8)\n",
    "\n",
    "            # Calculate coordinates to place the character in the center\n",
    "            x_offset = (w_padding - w) // 2\n",
    "            y_offset = (h_padding - h) // 2\n",
    "\n",
    "            # Copy the character region from the original image to padded image\n",
    "            padded_image[y_offset:y_offset+h, x_offset:x_offset+w] = image[y:y+h, x:x+w]\n",
    "\n",
    "            # Resize the padded image\n",
    "            resized_image = resize_image(padded_image, 100, 100)\n",
    "\n",
    "            # Save the resized image as a separate image\n",
    "            character_filename = os.path.join(output_directory, f'{image_name}_{char_labels[counter]}--{image_id}.png')\n",
    "            cv2.imwrite(character_filename, resized_image)\n",
    "            counter += 1 # adding one to the counter variable means that the method has found one of the four characters.\n",
    "            image_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing images to the filter (new image generation)\n",
    "\n",
    "This code snippet will run the dataset through the noise reduction filter, and store the outputs in a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate denoised images. Do not run this code snippet if folder and data already exists.\n",
    "folder = 'captcha_images'\n",
    "output_folder = 'denoised_images'\n",
    "\n",
    "# Get list of all files in the folder\n",
    "file_list = os.listdir(folder)\n",
    "\n",
    "# check if output directory exists\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Iterate through the first 10 images in the folder\n",
    "for i, filename in enumerate(file_list):    \n",
    "    # Check if the file is an image (you can add more image extensions if needed)\n",
    "    if filename.lower().endswith(('.png')):\n",
    "        # Construct the full path to the image\n",
    "        image_path = os.path.join(folder, filename)\n",
    "        \n",
    "        # Save path for the denoised image\n",
    "        # original image -> denoised image (now named after its label)\n",
    "        label = filename.split('-')[0]\n",
    "        save_filename = f'{label}_{i}--denoised.png'\n",
    "        save_path = os.path.join(output_folder, save_filename)\n",
    "        \n",
    "        # Call the remove_noise function\n",
    "        remove_noise(image_path, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cropping Characters (New Image Generation)\n",
    "This code snippet will run the dataset through the character selection filter, and store the outputs in a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Folder path containing the images\n",
    "folder_path = 'denoised_images'\n",
    "\n",
    "# Output directory for saved contour images\n",
    "output_directory = 'cropped_grayscale'\n",
    "\n",
    "# Get list of all files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# check if output directory exists\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "image_id = 0 # variable to keep track of each image's ID\n",
    "\n",
    "# Iterate through each image with no noise\n",
    "for i, filename in enumerate(file_list):\n",
    "    # Check if the file is an image\n",
    "    if filename.lower().endswith(('.png')):\n",
    "        # Construct the full path to the denoised image\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        # Extract areas of interest from denoised image\n",
    "        save_contours_as_images(image_path, output_directory, image_id)\n",
    "\n",
    "        image_id += 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Setup\n",
    "After all data has gone through the preprocessing and segmentation steps, the new data will then be transferred to a custom dataset. This dataset will be the one responsible for storing all the information needed for future training, such as <code>images</code> and <code>labels</code>.\n",
    "<br><br>\n",
    "The dataset will take two parameters:\n",
    "* <code>root_dir</code>: this is the folder where all the new data is located.\n",
    "* <code>transform</code>: this parameter stores all transformations that will be applied to the information in the dataset.\n",
    "\n",
    "For this task specifically, the data will be stored in a tuple that will use the following format: <code>(tensor([[image]]), label)</code>\n",
    "\n",
    "For reference, the code template used in this step can be found here: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "This link contains a template, the step by step process on how to build a custom dataset, and how to properly store the information in it.\n",
    "\n",
    "In this project, the data was generated by gathering samples of 35 different classes. They include digits that range from 1 to 9, and upper case characters that range from A to Z. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] # 35 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CroppedCharacterDataset(Dataset):\n",
    "    # Constructor\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir # where the data is located (folder)\n",
    "        self.transform = transform # a transforms.Compose() that contains all data transformations\n",
    "        # image paths are stored in an array for easy iteration over the data\n",
    "        self.image_paths = [os.path.join(root_dir, img) for img in os.listdir(root_dir) if img.endswith('.png')]\n",
    "\n",
    "    # returns dataset size\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    # returns a tensor that contains the image and the label of that image, given an index.\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('L')  # Convert image to grayscale\n",
    "        \n",
    "        # Extract label and image id from file name\n",
    "        filename = os.path.splitext(os.path.basename(img_path))[0]\n",
    "\n",
    "        # format used in the image file name creation: {ParentImageName}_{image_num}_{char_label}--{id}.png\n",
    "        parts = filename.split('_')\n",
    "        label, image_id = parts[-1].split('--')  # Split last part\n",
    "        label = label.strip()  # Remove any leading/trailing whitespace\n",
    "\n",
    "        # apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating instances and dataset visualization\n",
    "The following code snippet defines the transformations that will be applied to the dataset, creates an instance of a dataset, and shows how the data is accessed through the magic methods defined in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the dataset: 40415\n",
      "\n",
      "Example of a tensor stored in the dataset:\n",
      "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), 'S')\n"
     ]
    }
   ],
   "source": [
    "# root directory where images are stored\n",
    "root_dir = \"./cropped_grayscale\"\n",
    "\n",
    "# Define transformations. Here, since the previous filters already normalized all the data, \n",
    "# the only transformation that will be applied to the dataset will be turning it into tensors.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create dataset instance\n",
    "cropped_chars_dataset = CroppedCharacterDataset(root_dir, transform=transform)\n",
    "\n",
    "# Demonstration of how the data in the dataset is accessed and stored.\n",
    "print(f\"Number of images in the dataset: {len(cropped_chars_dataset)}\\n\") \n",
    "print(\"Example of a tensor stored in the dataset:\")\n",
    "print(cropped_chars_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Train and Test Sets\n",
    "This section covers how the train and test sets are defined. \n",
    "\n",
    "In this project, I used an 80%/20% split between both sets. Since the <code>cropped_chars_dataset</code> contains 40415 different images, this split is enough to train and evaluate the model later. \n",
    "\n",
    "The random splitting was done by storing the dataset size in separate variables defined <code>train_size</code> and <code>test_size</code>. With these variables, it was possible to call the <code>random_split</code> function that will do the job of randomly splitting the data into two different sets of the given sizes. \n",
    "\n",
    "After that, <code>Dataloaders</code> were created for each set. These will be used later in the training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 32332 examples into 1011 batches\n",
      "Test: 8083 examples into 253 batches\n"
     ]
    }
   ],
   "source": [
    "# Define the sizes of each split\n",
    "train_size = int(0.8 * len(cropped_chars_dataset)) # 80%\n",
    "test_size = len(cropped_chars_dataset) - train_size # 100% - 80% = 20%\n",
    "\n",
    "# Split dataset into train, and test sets (randomly)\n",
    "train_data, test_data = random_split(cropped_chars_dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True) \n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True) \n",
    "\n",
    "# Visualize training and dataset sizes\n",
    "print(f\"Train: {len(train_loader.dataset)} examples into {len(train_loader)} batches\")\n",
    "print(f\"Test: {len(test_loader.dataset)} examples into {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "Since this part of the project deals with image recognition and classification, a Convolutional Neural Network (CNN) is the perfect fit for this task. \n",
    "\n",
    "The model contains 3 different <code>conv2d</code> (convolutional) layers with a <code>padding</code> of 1, followed by 3x3 <code>kernels</code>. In between each layer, a 2x2 <code>MaxPool2d</code> was included as a way to downsample the spatial dimensions of the input feature and reduces their size while preserving important information. At the end, two fully-connected layers were added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes=35):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 12 * 12, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 12 * 12)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The following function executes one whole training run for a given model.\n",
    "\n",
    "The <code>train_model()</code> function takes the following parameters:\n",
    "\n",
    "* <code>model</code>: this is the model that will be used in training.\n",
    "* <code>train_loader</code>: the data loader that stores the data that will be passed to the model.\n",
    "* <code>num_epochs</code>: the number of epochs used in training.\n",
    "* <code>learning rate</code>: the learning rate used in training.\n",
    "* <code>loss_arr</code>: a python list that will store the loss values for each epoch that will be used later to plot the model's overall performance.\n",
    "* <code>acc_arr</code>: a python list that will store the accuracy values for each epoch that will be used later to plot the model's overall performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, num_epochs, learning_rate, loss_arr, acc_arr):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss() # Here, I used CrossEntropyLoss because it suits well when working with multiple-class classification.\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # For optimizer, among all optimizers that I tested in the model, Adam worked best.\n",
    "    \n",
    "    # Training run\n",
    "    for epoch in range(num_epochs):\n",
    "        # Variables that will keep track of model's performance \n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Extract and convert labels to integers\n",
    "            label_strings = [label_tuple[0] for label_tuple in labels]\n",
    "            label_indices = [classes.index(label) for label in label_strings]\n",
    "            \n",
    "            # creating tensor of labels\n",
    "            labels = torch.tensor(label_indices, dtype=torch.long).to(device)\n",
    "            \n",
    "            # zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward propagation\n",
    "            outputs = model(images)\n",
    "\n",
    "            # computer loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # backward prop\n",
    "            loss.backward()\n",
    "\n",
    "            # update\n",
    "            optimizer.step()\n",
    "            \n",
    "            # calculating stats\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = correct / total\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {100 * train_accuracy:.2f}%\")\n",
    "        \n",
    "        loss_arr.append(train_loss)\n",
    "        acc_arr.append(train_accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "The following function tests the model on the test set using a model.\n",
    "\n",
    "The <code>test_model()</code> function takes the following parameters:\n",
    "\n",
    "* <code>model</code>: this is the model that will be used in training.\n",
    "* <code>test_loader</code>: the data loader that stores the data that will be passed to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss() # Here, I used CrossEntropyLoss because it suits well when working with multiple class classification.\n",
    "\n",
    "    # These are variables that will be used to calculate the training statistics\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Iterating through data loader\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device) # Extract and convert labels to integers\n",
    "            \n",
    "            # Converting \n",
    "            label_strings = [label_tuple[0] for label_tuple in labels]\n",
    "            label_indices = [classes.index(label) for label in label_strings]\n",
    "            labels = torch.tensor(label_indices, dtype=torch.long).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate loss and accuracy\n",
    "    test_loss = running_loss / len(test_loader.dataset)\n",
    "    test_accuracy = total_correct / total\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {100 * test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instances and hyperparameters\n",
    "All instances and hyperparameters for training are defined here. Values used were acquired through hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model...\n",
      "Epoch [1/10], Train Loss: 0.4515, Train Accuracy: 87.03%\n",
      "Epoch [2/10], Train Loss: 0.0501, Train Accuracy: 98.58%\n",
      "Epoch [3/10], Train Loss: 0.0303, Train Accuracy: 99.12%\n",
      "Epoch [4/10], Train Loss: 0.0213, Train Accuracy: 99.39%\n",
      "Epoch [5/10], Train Loss: 0.0132, Train Accuracy: 99.60%\n"
     ]
    }
   ],
   "source": [
    "# Create model instance\n",
    "model = CNNModel()\n",
    "\n",
    "# Define number of epochs and learning rate \n",
    "epochs = 10\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# python lists to store information that will be used to plot the results.\n",
    "losses_train = []\n",
    "accuracies_train = []\n",
    "\n",
    "# Check accuracies on both test and training datasets\n",
    "print(\"Training Model...\")\n",
    "train_model(model, train_loader, epochs, learning_rate, losses_train, accuracies_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model\n",
    "\n",
    "Once the model parameters have been adjusted, I tested the model's accuracy on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Model...\")\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will show the results from the training runs for a given model.\n",
    "\n",
    "For this, I used the <code>matplotlib.pyplot</code> module, commonly used in data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses_train, color=\"green\", label='Training Loss', linewidth=2.0) \n",
    "plt.title('Overall Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Annotate last value of loss on the plot\n",
    "plt.annotate(f'{losses_train[-1]:.3f}', xy=(len(losses_train)-1, losses_train[-1]), xytext=(-20, 10), textcoords='offset points', color='green')\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracies_train, color=\"green\", label='Training Accuracy', linewidth=2.0) \n",
    "plt.title('Overall Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Annotate last value of accuracy on the plot\n",
    "plt.annotate(f'{accuracies_train[-1]:.2%}', xy=(len(accuracies_train)-1, accuracies_train[-1]), xytext=(-30, -15), textcoords='offset points', color='green')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Analysis\n",
    "\n",
    "After some training runs, I was able to reach an overall accuracy of 99.85%, and an overall loss value of 0.005, which were values above the expected. As a way to measure how well the model performs, I calculated the human-level performance in this task. In the given dataset, I was able to accurately solve 9800 images (or 98% of the dataset). It was quite intriguing to see that the model performed better than I did.  \n",
    "\n",
    "While working on the project, I had different expectations on how well the model would work. Before coming up with a concrete project proposal, I thought that models would have a hard time solving the text-based captcha puzzles because these puzzles were designed to prevent computers from accessing confidential data. At first, I expected the model to reach quite poor accuracy (<50%). Doing some research on how to approach this problem and coming up with a project proposal, I could see that by breaking the problem into smaller steps and turning it into a simple classification task, I could reach more satisfying results. During this time, I was expecting the model to reach an overall accuracy a little below 90% because I still thought that the distortion and random patterns in the images were going to be an obstacle. By the time I trained the model, I was surprised to see that it performed better than a human, reaching results above the expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "The development and implementation of the captcha recognition model demonstrated the efficacy of deep learning in solving complex image recognition tasks. Through the use of a convolutional neural networks (CNNs) and data preprocessing, I was able to achieve a high level of accuracy in solving these puzzles. The model's ability to generalize to new, unseen, distorted captcha characters shows its robustness and potential for a real-world application. \n",
    "\n",
    "\n",
    "### Future work\n",
    "While I was able to build a fully functional model, I strongly believe there is still plenty of research to do. \n",
    "\n",
    "Thanks to advancements in Computer Science, end-to-end models have been quite common in solving tasks of many different types. There are approaches to character recognition that involve sequential models that could definitely be used in a task like captcha recognition. Models that implement CNNs and RNNs with attention have reached great results (>90% accuracy) in reading text from images. My plans for future work is to develop an end-to-end model that uses a sequential neural network, aiming to reach an overall accuracy above 90%, possibly reaching results like the ones in this project (>95% accuracy). \n",
    "\n",
    "While I could not build a fully functional end-to-end model now, the research I did has shown me that it is possible to attack this problem and reach accuracies above 90%. Some more research must be done, and I will spend the next weeks and months working on this.\n",
    "\n",
    "### What I have been learning for future work\n",
    "Here is an article that shows a little of what I have been learning: https://codingvision.net/pytorch-crnn-seq2seq-digits-recognition-ctc. This article shows an approach to reading distorted handwritten characters from images using a sequential model (GRU). I believe that a similar approach can be used in captcha recognition tasks. In this case, the model learns how to distinguish noise and characters, so no noise reduction filter and character segmentation functions are needed to improve performance.\n",
    "\n",
    "Another really interesting article that implements an end-to-end captcha model is this one: https://www.sciencedirect.com/science/article/pii/S0925231220318518. What makes this approach even more interesting is that the model involves Generative Adversarial Networks (GANs) to synthesize data and make the dataset larger, the use of a Bi-Directional Long-Short Term Memory model instead of Gated Recurrent Units (GRUs), and the ability to solve many types of text-based captcha images. The model in this approach is more complex, but it is able to generalize really well on many different distortions and patterns.\n",
    "\n",
    "During the summer, I will be diving a little deeper into these articles and build an end-to-end model that is able to solve text-based captcha puzzles. I strongly believe that extending this project to an end-to-end model will deepen my understanding of more advanced techniques and approaches that will be useful later in academic projects and future career. \n",
    "\n",
    "### What the project has taught me\n",
    "\n",
    "Overall, this project has not only deepened my understanding of deep learning principles learned throughout the semester but also highlighted its practical utility in solving challenging image recognition problems. Working on this project has taught me about how computer vision works, its importance, and how much can be done with it. It has also shown me the iterative nature of model development, where constant experimentation and evaluation are necessary to achieve good performance. Moreover, this project has taught me about how to create and handle my own dataset, and the significance of its quality during the preprocessing steps to normalize the data. Also, I was able to put the concept of hyperparameter tuning and error analysis into practice, two things that I learned in class this semester but never had the chance to do it \"from scratch\" in a big project. This project has deepened my appreciation for the complexity and potential of deep learning in real-world applications. It has definitely changed my mind career wise. It is a challenging field to work on, but personally, it is rewarding when I see a project like this one working. I would definitely pursue a career in AI/machine learning/deep learning thanks to this class and this project. \n",
    "\n",
    "When it comes to captchas, working on this project has made me reflect on how captcha puzzles in the future must change in order to prevent computers from accessing web pages that can contain confidential data. With the right tools and knowledge, some text-based captchas can be solved with ease, so the implementation of more challenging captcha formats, such as image-based captchas, and drag-and-drop captchas can provide a more resistant barrier between robots and data, preventing any data from being compromised. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
