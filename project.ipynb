{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Captcha Recognition Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\luizf\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\luizf\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opencv-python) (1.26.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\luizf\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.26.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset, random_split\n",
    "from torch import optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import math\n",
    "import random # sampling captcha text\n",
    "import os # used for path and image storage\n",
    "from captcha.image import ImageCaptcha  # Module that will generate all captcha images# pip install captcha\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device to point to a GPU if we have one, CPU otherwise.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(image_path, save_path):\n",
    "    # Open the image using Pillow\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray_image = image.convert('L')\n",
    "\n",
    "    # Convert PIL image to numpy array\n",
    "    np_image = np.array(gray_image)\n",
    "\n",
    "    # Apply binary thresholding\n",
    "    _, binary_image = cv2.threshold(np_image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Apply morphological operations (optional)\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    opening = cv2.morphologyEx(binary_image, cv2.MORPH_OPEN, kernel, iterations=1)  # Increase iterations for more noise removal\n",
    "\n",
    "    # Invert the binary image\n",
    "    inverted_image = cv2.bitwise_not(opening)\n",
    "\n",
    "    # Save the inverted binary image\n",
    "    inverted_image_pil = Image.fromarray(inverted_image)\n",
    "\n",
    "    # Invert the colors again to have black background and white letters\n",
    "    inverted_image_pil = inverted_image_pil.convert('L')\n",
    "    inverted_image = np.array(inverted_image_pil)\n",
    "    inverted_image = cv2.bitwise_not(inverted_image)\n",
    "\n",
    "    # Save the final image\n",
    "    final_image = Image.fromarray(inverted_image)\n",
    "    final_image.save(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to resize the image\n",
    "def resize_image(image, new_width, new_height):\n",
    "    # Resize the image\n",
    "    resized_image = cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "    # Apply binary thresholding to ensure black and white pixels only\n",
    "    _, binary_image = cv2.threshold(resized_image, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "    # Return the final inverted image\n",
    "    return binary_image\n",
    "\n",
    "def save_contours_as_images(image_path, output_directory, image_id):\n",
    "    # Load the image in grayscale\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Threshold the image to obtain binary image\n",
    "    _, binary_image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY)\n",
    "#\n",
    "    ## Find contours in the binary image\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Sort contours based on x-coordinate\n",
    "    contours = sorted(contours, key=lambda contour: cv2.boundingRect(contour)[0])\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    counter = 0 # keep track of how many characters have been saved\n",
    "    label = image_path.split('/')[0].split('.')[0].split(\"\\\\\")[1]\n",
    "    # print(label)\n",
    "    image_name = label.split(\"--\")[0]\n",
    "    char_labels = [char_label for char_label in label.split(\"_\")[0]]\n",
    "    # print(char_labels)\n",
    "    \n",
    "    for i, contour in enumerate(contours):\n",
    "        # Get bounding box for each contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "        if counter > 3:\n",
    "            break\n",
    "\n",
    "        # Check if contour is too small (possibly noise)\n",
    "        if w > 5 and h > 5:\n",
    "            # Add some padding around the character bounding box\n",
    "            padding = 10\n",
    "            x_padding = max(0, x - padding)\n",
    "            y_padding = max(0, y - padding)\n",
    "            w_padding = min(image.shape[1], w + 2 * padding)\n",
    "            h_padding = min(image.shape[0], h + 2 * padding)\n",
    "\n",
    "            # Create a black canvas with padded dimensions\n",
    "            padded_image = np.zeros((h_padding, w_padding), dtype=np.uint8)\n",
    "\n",
    "            # Calculate coordinates to place the character in the center\n",
    "            x_offset = (w_padding - w) // 2\n",
    "            y_offset = (h_padding - h) // 2\n",
    "\n",
    "            # Copy the character region from the original image to padded image\n",
    "            padded_image[y_offset:y_offset+h, x_offset:x_offset+w] = image[y:y+h, x:x+w]\n",
    "\n",
    "            # Resize the padded image\n",
    "            # Resize method is adding a smooth image.\n",
    "            resized_image = resize_image(padded_image, 100, 100)\n",
    "\n",
    "            # Save the resized image as a separate image\n",
    "            character_filename = os.path.join(output_directory, f'{image_name}_{char_labels[counter]}--{image_id}.png')\n",
    "            cv2.imwrite(character_filename, resized_image)\n",
    "            print(f\"contour saved: {character_filename}\")\n",
    "            counter += 1    \n",
    "            image_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing images to the filter (new image generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate denoised images\n",
    "folder = 'four_cap_36'\n",
    "output_folder = 'denoised_images'\n",
    "\n",
    "# Get list of all files in the folder\n",
    "file_list = os.listdir(folder)\n",
    "\n",
    "# Iterate through the first 10 images in the folder\n",
    "for i, filename in enumerate(file_list):    \n",
    "    # Check if the file is an image (you can add more image extensions if needed)\n",
    "    if filename.lower().endswith(('.png')):\n",
    "        # Construct the full path to the image\n",
    "        image_path = os.path.join(folder, filename)\n",
    "        \n",
    "        # Save path for the denoised image\n",
    "        # original image -> denoised image (now named after its label)\n",
    "        label = filename.split('-')[0]\n",
    "        save_filename = f'{label}_{i}--denoised.png'\n",
    "        save_path = os.path.join(output_folder, save_filename)\n",
    "        \n",
    "        # Call the remove_noise function\n",
    "        remove_noise(image_path, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cropping Characters (New Image Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contour saved: cropped_characters\\17U3_0_1--0.png\n",
      "contour saved: cropped_characters\\17U3_0_7--1.png\n",
      "contour saved: cropped_characters\\17U3_0_U--2.png\n",
      "contour saved: cropped_characters\\17U3_0_3--3.png\n",
      "contour saved: cropped_characters\\1BG4_1_1--4.png\n",
      "contour saved: cropped_characters\\1BG4_1_B--5.png\n",
      "contour saved: cropped_characters\\1BG4_1_G--6.png\n",
      "contour saved: cropped_characters\\1BG4_1_4--7.png\n",
      "contour saved: cropped_characters\\1IJA_2_1--8.png\n",
      "contour saved: cropped_characters\\1IJA_2_I--9.png\n",
      "contour saved: cropped_characters\\1IJA_2_J--10.png\n",
      "contour saved: cropped_characters\\1IJA_2_A--11.png\n",
      "contour saved: cropped_characters\\1MC6_3_1--12.png\n",
      "contour saved: cropped_characters\\1MC6_3_M--13.png"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "contour saved: cropped_characters\\1MC6_3_C--14.png\n",
      "contour saved: cropped_characters\\1MC6_3_6--15.png\n",
      "contour saved: cropped_characters\\1O2U_4_1--16.png\n",
      "contour saved: cropped_characters\\1O2U_4_O--17.png\n",
      "contour saved: cropped_characters\\1O2U_4_2--18.png\n",
      "contour saved: cropped_characters\\1O2U_4_U--19.png\n",
      "contour saved: cropped_characters\\21LM_5_2--20.png\n",
      "contour saved: cropped_characters\\21LM_5_1--21.png\n",
      "contour saved: cropped_characters\\21LM_5_L--22.png\n",
      "contour saved: cropped_characters\\21LM_5_M--23.png\n",
      "contour saved: cropped_characters\\2DXI_6_2--24.png\n",
      "contour saved: cropped_characters\\2DXI_6_D--25.png\n",
      "contour saved: cropped_characters\\2DXI_6_X--26.png\n",
      "contour saved: cropped_characters\\2DXI_6_I--27.png\n",
      "contour saved: cropped_characters\\2NPF_7_2--28.png\n",
      "contour saved: cropped_characters\\2NPF_7_N--29.png\n",
      "contour saved: cropped_characters\\2NPF_7_P--30.png\n",
      "contour saved: cropped_characters\\2NPF_7_F--31.png\n",
      "contour saved: cropped_characters\\2P57_8_2--32.png\n",
      "contour saved: cropped_characters\\2P57_8_P--33.png\n",
      "contour saved: cropped_characters\\2P57_8_5--34.png\n",
      "contour saved: cropped_characters\\2P57_8_7--35.png\n",
      "contour saved: cropped_characters\\2XU3_9_2--36.png\n",
      "contour saved: cropped_characters\\2XU3_9_X--37.png\n",
      "contour saved: cropped_characters\\2XU3_9_U--38.png\n",
      "contour saved: cropped_characters\\2XU3_9_3--39.png\n",
      "contour saved: cropped_characters\\32FD_10_3--40.png\n",
      "contour saved: cropped_characters\\32FD_10_2--41.png\n",
      "contour saved: cropped_characters\\32FD_10_F--42.png\n",
      "contour saved: cropped_characters\\32FD_10_D--43.png\n",
      "contour saved: cropped_characters\\3HGA_12_3--44.png\n",
      "contour saved: cropped_characters\\3HGA_12_H--45.png\n",
      "contour saved: cropped_characters\\3HGA_12_G--46.png\n",
      "contour saved: cropped_characters\\3HGA_12_A--47.png\n",
      "contour saved: cropped_characters\\3L3D_13_3--48.png\n",
      "contour saved: cropped_characters\\3L3D_13_L--49.png\n",
      "contour saved: cropped_characters\\3L3D_13_3--50.png\n",
      "contour saved: cropped_characters\\3L3D_13_D--51.png\n",
      "contour saved: cropped_characters\\3U8W_14_3--52.png\n",
      "contour saved: cropped_characters\\3U8W_14_U--53.png\n",
      "contour saved: cropped_characters\\3U8W_14_8--54.png\n",
      "contour saved: cropped_characters\\3U8W_14_W--55.png\n",
      "contour saved: cropped_characters\\3UVB_15_3--56.png\n",
      "contour saved: cropped_characters\\3UVB_15_U--57.png\n",
      "contour saved: cropped_characters\\3UVB_15_V--58.png\n",
      "contour saved: cropped_characters\\3UVB_15_B--59.png\n",
      "contour saved: cropped_characters\\449J_16_4--60.png\n",
      "contour saved: cropped_characters\\449J_16_4--61.png\n",
      "contour saved: cropped_characters\\449J_16_9--62.png\n",
      "contour saved: cropped_characters\\449J_16_J--63.png\n",
      "contour saved: cropped_characters\\45P2_17_4--64.png\n",
      "contour saved: cropped_characters\\45P2_17_5--65.png\n",
      "contour saved: cropped_characters\\45P2_17_P--66.png\n",
      "contour saved: cropped_characters\\45P2_17_2--67.png\n",
      "contour saved: cropped_characters\\468B_18_4--68.png\n",
      "contour saved: cropped_characters\\468B_18_6--69.png\n",
      "contour saved: cropped_characters\\468B_18_8--70.png\n",
      "contour saved: cropped_characters\\468B_18_B--71.png\n",
      "contour saved: cropped_characters\\4A6H_19_4--72.png\n",
      "contour saved: cropped_characters\\4A6H_19_A--73.png\n",
      "contour saved: cropped_characters\\4A6H_19_6--74.png\n",
      "contour saved: cropped_characters\\4A6H_19_H--75.png\n",
      "contour saved: cropped_characters\\4DHW_20_4--76.png\n",
      "contour saved: cropped_characters\\4DHW_20_D--77.png\n",
      "contour saved: cropped_characters\\4DHW_20_H--78.png\n",
      "contour saved: cropped_characters\\4DHW_20_W--79.png\n",
      "contour saved: cropped_characters\\4JW1_21_4--80.png\n",
      "contour saved: cropped_characters\\4JW1_21_J--81.png\n",
      "contour saved: cropped_characters\\4JW1_21_W--82.png\n",
      "contour saved: cropped_characters\\4JW1_21_1--83.png\n",
      "contour saved: cropped_characters\\4LCR_22_4--84.png\n",
      "contour saved: cropped_characters\\4LCR_22_L--85.png\n",
      "contour saved: cropped_characters\\4LCR_22_C--86.png\n",
      "contour saved: cropped_characters\\4LCR_22_R--87.png\n",
      "contour saved: cropped_characters\\4V39_23_4--88.png\n",
      "contour saved: cropped_characters\\4V39_23_V--89.png\n",
      "contour saved: cropped_characters\\4V39_23_3--90.png\n",
      "contour saved: cropped_characters\\4V39_23_9--91.png\n",
      "contour saved: cropped_characters\\4YU6_24_4--92.png\n",
      "contour saved: cropped_characters\\4YU6_24_Y--93.png\n",
      "contour saved: cropped_characters\\4YU6_24_U--94.png\n",
      "contour saved: cropped_characters\\4YU6_24_6--95.png\n",
      "contour saved: cropped_characters\\511W_25_5--96.png\n",
      "contour saved: cropped_characters\\511W_25_1--97.png\n",
      "contour saved: cropped_characters\\511W_25_1--98.png\n",
      "contour saved: cropped_characters\\531F_26_5--100.png\n",
      "contour saved: cropped_characters\\531F_26_3--101.png\n",
      "contour saved: cropped_characters\\531F_26_1--102.png\n",
      "contour saved: cropped_characters\\5ORC_28_5--104.png\n",
      "contour saved: cropped_characters\\5ORC_28_O--105.png\n",
      "contour saved: cropped_characters\\5ORC_28_R--106.png\n",
      "contour saved: cropped_characters\\5ORC_28_C--107.png\n",
      "contour saved: cropped_characters\\5PW9_29_5--108.png\n",
      "contour saved: cropped_characters\\5PW9_29_P--109.png\n",
      "contour saved: cropped_characters\\5PW9_29_W--110.png\n",
      "contour saved: cropped_characters\\5PW9_29_9--111.png\n",
      "contour saved: cropped_characters\\5QN5_30_5--112.png\n",
      "contour saved: cropped_characters\\5QN5_30_Q--113.png\n",
      "contour saved: cropped_characters\\5QN5_30_N--114.png\n",
      "contour saved: cropped_characters\\5QN5_30_5--115.png\n",
      "contour saved: cropped_characters\\5RVB_31_5--116.png\n",
      "contour saved: cropped_characters\\5RVB_31_R--117.png\n",
      "contour saved: cropped_characters\\5RVB_31_V--118.png\n",
      "contour saved: cropped_characters\\5RVB_31_B--119.png\n",
      "contour saved: cropped_characters\\5YYE_32_5--120.png\n",
      "contour saved: cropped_characters\\5YYE_32_Y--121.png\n",
      "contour saved: cropped_characters\\5YYE_32_Y--122.png\n",
      "contour saved: cropped_characters\\5YYE_32_E--123.png\n",
      "contour saved: cropped_characters\\61Y3_33_6--124.png\n",
      "contour saved: cropped_characters\\61Y3_33_1--125.png\n",
      "contour saved: cropped_characters\\61Y3_33_Y--126.png\n",
      "contour saved: cropped_characters\\61Y3_33_3--127.png\n",
      "contour saved: cropped_characters\\6EB7_34_6--128.png\n",
      "contour saved: cropped_characters\\6EB7_34_E--129.png\n",
      "contour saved: cropped_characters\\6EB7_34_B--130.png\n",
      "contour saved: cropped_characters\\6EB7_34_7--131.png\n",
      "contour saved: cropped_characters\\6PBW_35_6--132.png\n",
      "contour saved: cropped_characters\\6PBW_35_P--133.png\n",
      "contour saved: cropped_characters\\6PBW_35_B--134.png\n",
      "contour saved: cropped_characters\\6PBW_35_W--135.png\n",
      "contour saved: cropped_characters\\6TOY_36_6--136.png\n",
      "contour saved: cropped_characters\\6TOY_36_T--137.png\n",
      "contour saved: cropped_characters\\6TOY_36_O--138.png\n",
      "contour saved: cropped_characters\\6TOY_36_Y--139.png\n",
      "contour saved: cropped_characters\\6VK1_37_6--140.png\n",
      "contour saved: cropped_characters\\6VK1_37_V--141.png\n",
      "contour saved: cropped_characters\\6VK1_37_K--142.png\n",
      "contour saved: cropped_characters\\6VK1_37_1--143.png\n",
      "contour saved: cropped_characters\\6VR3_38_6--144.png\n",
      "contour saved: cropped_characters\\6VR3_38_V--145.png\n",
      "contour saved: cropped_characters\\6VR3_38_R--146.png\n",
      "contour saved: cropped_characters\\6VR3_38_3--147.png\n",
      "contour saved: cropped_characters\\76ME_39_7--148.png\n",
      "contour saved: cropped_characters\\76ME_39_6--149.png\n",
      "contour saved: cropped_characters\\76ME_39_M--150.png\n",
      "contour saved: cropped_characters\\76ME_39_E--151.png\n",
      "contour saved: cropped_characters\\79C7_40_7--152.png\n",
      "contour saved: cropped_characters\\79C7_40_9--153.png\n",
      "contour saved: cropped_characters\\79C7_40_C--154.png\n",
      "contour saved: cropped_characters\\79C7_40_7--155.png\n",
      "contour saved: cropped_characters\\7FO2_41_7--156.png\n",
      "contour saved: cropped_characters\\7FO2_41_F--157.png\n",
      "contour saved: cropped_characters\\7FO2_41_O--158.png\n",
      "contour saved: cropped_characters\\7FO2_41_2--159.png\n",
      "contour saved: cropped_characters\\7LVF_42_7--160.png\n",
      "contour saved: cropped_characters\\7LVF_42_L--161.png\n",
      "contour saved: cropped_characters\\7LVF_42_V--162.png\n",
      "contour saved: cropped_characters\\7LVF_42_F--163.png\n",
      "contour saved: cropped_characters\\7MJ8_43_7--164.png\n",
      "contour saved: cropped_characters\\7MJ8_43_M--165.png\n",
      "contour saved: cropped_characters\\7MJ8_43_J--166.png\n",
      "contour saved: cropped_characters\\7MJ8_43_8--167.png\n",
      "contour saved: cropped_characters\\7MOY_44_7--168.png\n",
      "contour saved: cropped_characters\\7MOY_44_M--169.png\n",
      "contour saved: cropped_characters\\7MOY_44_O--170.png\n",
      "contour saved: cropped_characters\\7MOY_44_Y--171.png\n",
      "contour saved: cropped_characters\\7MY9_45_7--172.png\n",
      "contour saved: cropped_characters\\7MY9_45_M--173.png\n",
      "contour saved: cropped_characters\\7MY9_45_Y--174.png\n",
      "contour saved: cropped_characters\\7MY9_45_9--175.png\n",
      "contour saved: cropped_characters\\84AR_46_8--176.png\n",
      "contour saved: cropped_characters\\84AR_46_4--177.png\n",
      "contour saved: cropped_characters\\84AR_46_A--178.png\n",
      "contour saved: cropped_characters\\84AR_46_R--179.png\n",
      "contour saved: cropped_characters\\8765_47_8--180.png\n",
      "contour saved: cropped_characters\\8765_47_7--181.png\n",
      "contour saved: cropped_characters\\8765_47_6--182.png\n",
      "contour saved: cropped_characters\\8765_47_5--183.png\n",
      "contour saved: cropped_characters\\8ASA_48_8--184.png\n",
      "contour saved: cropped_characters\\8ASA_48_A--185.png\n",
      "contour saved: cropped_characters\\8ASA_48_S--186.png\n",
      "contour saved: cropped_characters\\8ASA_48_A--187.png\n",
      "contour saved: cropped_characters\\8CEW_49_8--188.png\n",
      "contour saved: cropped_characters\\8CEW_49_C--189.png\n",
      "contour saved: cropped_characters\\8CEW_49_E--190.png\n",
      "contour saved: cropped_characters\\8CEW_49_W--191.png\n",
      "contour saved: cropped_characters\\8CMZ_50_8--192.png\n",
      "contour saved: cropped_characters\\8CMZ_50_C--193.png\n",
      "contour saved: cropped_characters\\8CMZ_50_M--194.png\n",
      "contour saved: cropped_characters\\8CMZ_50_Z--195.png\n",
      "contour saved: cropped_characters\\8IZZ_51_8--196.png\n",
      "contour saved: cropped_characters\\8IZZ_51_I--197.png\n",
      "contour saved: cropped_characters\\8IZZ_51_Z--198.png\n",
      "contour saved: cropped_characters\\8IZZ_51_Z--199.png\n",
      "contour saved: cropped_characters\\8OCX_52_8--200.png\n",
      "contour saved: cropped_characters\\8OCX_52_O--201.png\n",
      "contour saved: cropped_characters\\8OCX_52_C--202.png\n",
      "contour saved: cropped_characters\\8OCX_52_X--203.png\n",
      "contour saved: cropped_characters\\9C1G_53_9--204.png\n",
      "contour saved: cropped_characters\\9C1G_53_C--205.png\n",
      "contour saved: cropped_characters\\9C1G_53_1--206.png\n",
      "contour saved: cropped_characters\\9C1G_53_G--207.png\n",
      "contour saved: cropped_characters\\9CQH_54_9--208.png\n",
      "contour saved: cropped_characters\\9CQH_54_C--209.png\n",
      "contour saved: cropped_characters\\9CQH_54_Q--210.png\n",
      "contour saved: cropped_characters\\9CQH_54_H--211.png\n",
      "contour saved: cropped_characters\\9VNL_55_9--212.png\n",
      "contour saved: cropped_characters\\9VNL_55_V--213.png\n",
      "contour saved: cropped_characters\\9VNL_55_N--214.png\n",
      "contour saved: cropped_characters\\9VNL_55_L--215.png\n",
      "contour saved: cropped_characters\\9YKW_56_9--216.png\n",
      "contour saved: cropped_characters\\9YKW_56_Y--217.png\n",
      "contour saved: cropped_characters\\9YKW_56_K--218.png\n",
      "contour saved: cropped_characters\\9YKW_56_W--219.png\n",
      "contour saved: cropped_characters\\A3D6_57_A--220.png\n",
      "contour saved: cropped_characters\\A3D6_57_3--221.png\n",
      "contour saved: cropped_characters\\A3D6_57_D--222.png\n",
      "contour saved: cropped_characters\\A3D6_57_6--223.png\n",
      "contour saved: cropped_characters\\A474_58_A--224.png\n",
      "contour saved: cropped_characters\\A474_58_4--225.png\n",
      "contour saved: cropped_characters\\A474_58_7--226.png\n",
      "contour saved: cropped_characters\\A474_58_4--227.png\n",
      "contour saved: cropped_characters\\A9CP_59_A--228.png\n",
      "contour saved: cropped_characters\\A9CP_59_9--229.png\n",
      "contour saved: cropped_characters\\A9CP_59_C--230.png\n",
      "contour saved: cropped_characters\\A9CP_59_P--231.png\n",
      "contour saved: cropped_characters\\AIXR_60_A--232.png\n",
      "contour saved: cropped_characters\\AIXR_60_I--233.png\n",
      "contour saved: cropped_characters\\AIXR_60_X--234.png\n",
      "contour saved: cropped_characters\\AIXR_60_R--235.png\n",
      "contour saved: cropped_characters\\AM5V_61_A--236.png\n",
      "contour saved: cropped_characters\\AM5V_61_M--237.png\n",
      "contour saved: cropped_characters\\AM5V_61_5--238.png\n",
      "contour saved: cropped_characters\\AM5V_61_V--239.png\n",
      "contour saved: cropped_characters\\B3O4_62_B--240.png\n",
      "contour saved: cropped_characters\\B3O4_62_3--241.png\n",
      "contour saved: cropped_characters\\B3O4_62_O--242.png\n",
      "contour saved: cropped_characters\\B3O4_62_4--243.png\n",
      "contour saved: cropped_characters\\BHEK_63_B--244.png\n",
      "contour saved: cropped_characters\\BHEK_63_H--245.png\n",
      "contour saved: cropped_characters\\BHEK_63_E--246.png\n",
      "contour saved: cropped_characters\\BHEK_63_K--247.png\n",
      "contour saved: cropped_characters\\BTAI_64_B--248.png\n",
      "contour saved: cropped_characters\\BTAI_64_T--249.png\n",
      "contour saved: cropped_characters\\BTAI_64_A--250.png\n",
      "contour saved: cropped_characters\\BTAI_64_I--251.png\n",
      "contour saved: cropped_characters\\BVDD_65_B--252.png\n",
      "contour saved: cropped_characters\\BVDD_65_V--253.png\n",
      "contour saved: cropped_characters\\BVDD_65_D--254.png\n",
      "contour saved: cropped_characters\\BVDD_65_D--255.png\n",
      "contour saved: cropped_characters\\CYFF_66_C--256.png\n",
      "contour saved: cropped_characters\\CYFF_66_Y--257.png\n",
      "contour saved: cropped_characters\\CYFF_66_F--258.png\n",
      "contour saved: cropped_characters\\CYFF_66_F--259.png\n",
      "contour saved: cropped_characters\\D637_67_D--260.png\n",
      "contour saved: cropped_characters\\D637_67_6--261.png\n",
      "contour saved: cropped_characters\\D637_67_3--262.png\n",
      "contour saved: cropped_characters\\D637_67_7--263.png\n",
      "contour saved: cropped_characters\\E74C_68_E--264.png\n",
      "contour saved: cropped_characters\\E74C_68_7--265.png\n",
      "contour saved: cropped_characters\\E74C_68_4--266.png\n",
      "contour saved: cropped_characters\\E74C_68_C--267.png\n",
      "contour saved: cropped_characters\\E91G_69_E--268.png\n",
      "contour saved: cropped_characters\\E91G_69_9--269.png\n",
      "contour saved: cropped_characters\\E91G_69_1--270.png\n",
      "contour saved: cropped_characters\\E91G_69_G--271.png\n",
      "contour saved: cropped_characters\\EBXZ_70_E--272.png\n",
      "contour saved: cropped_characters\\EBXZ_70_B--273.png\n",
      "contour saved: cropped_characters\\EBXZ_70_X--274.png\n",
      "contour saved: cropped_characters\\EBXZ_70_Z--275.png\n",
      "contour saved: cropped_characters\\EG5D_71_E--276.png\n",
      "contour saved: cropped_characters\\EG5D_71_G--277.png\n",
      "contour saved: cropped_characters\\EG5D_71_5--278.png\n",
      "contour saved: cropped_characters\\EG5D_71_D--279.png\n",
      "contour saved: cropped_characters\\F17U_72_F--280.png\n",
      "contour saved: cropped_characters\\F17U_72_1--281.png\n",
      "contour saved: cropped_characters\\F17U_72_7--282.png\n",
      "contour saved: cropped_characters\\F17U_72_U--283.png\n",
      "contour saved: cropped_characters\\F28H_73_F--284.png\n",
      "contour saved: cropped_characters\\F28H_73_2--285.png\n",
      "contour saved: cropped_characters\\F28H_73_8--286.png\n",
      "contour saved: cropped_characters\\F28H_73_H--287.png\n",
      "contour saved: cropped_characters\\FFSA_74_F--288.png\n",
      "contour saved: cropped_characters\\FFSA_74_F--289.png\n",
      "contour saved: cropped_characters\\FFSA_74_S--290.png\n",
      "contour saved: cropped_characters\\FFSA_74_A--291.png\n",
      "contour saved: cropped_characters\\FNIT_75_F--292.png\n",
      "contour saved: cropped_characters\\FNIT_75_N--293.png\n",
      "contour saved: cropped_characters\\FNIT_75_I--294.png\n",
      "contour saved: cropped_characters\\FNIT_75_T--295.png\n",
      "contour saved: cropped_characters\\FNJD_76_F--296.png\n",
      "contour saved: cropped_characters\\FNJD_76_N--297.png\n",
      "contour saved: cropped_characters\\FNJD_76_J--298.png\n",
      "contour saved: cropped_characters\\FNJD_76_D--299.png\n",
      "contour saved: cropped_characters\\FOO6_77_F--300.png\n",
      "contour saved: cropped_characters\\FOO6_77_O--301.png\n",
      "contour saved: cropped_characters\\FOO6_77_O--302.png\n",
      "contour saved: cropped_characters\\FOO6_77_6--303.png\n",
      "contour saved: cropped_characters\\FT4S_78_F--304.png\n",
      "contour saved: cropped_characters\\FT4S_78_T--305.png\n",
      "contour saved: cropped_characters\\FT4S_78_4--306.png\n",
      "contour saved: cropped_characters\\FT4S_78_S--307.png\n",
      "contour saved: cropped_characters\\FXX4_79_F--308.png\n",
      "contour saved: cropped_characters\\FXX4_79_X--309.png\n",
      "contour saved: cropped_characters\\FXX4_79_X--310.png\n",
      "contour saved: cropped_characters\\FXX4_79_4--311.png\n",
      "contour saved: cropped_characters\\G6ET_80_G--312.png\n",
      "contour saved: cropped_characters\\G6ET_80_6--313.png\n",
      "contour saved: cropped_characters\\G6ET_80_E--314.png\n",
      "contour saved: cropped_characters\\G6ET_80_T--315.png\n",
      "contour saved: cropped_characters\\GKAQ_81_G--316.png\n",
      "contour saved: cropped_characters\\GKAQ_81_K--317.png\n",
      "contour saved: cropped_characters\\GKAQ_81_A--318.png\n",
      "contour saved: cropped_characters\\GKAQ_81_Q--319.png\n",
      "contour saved: cropped_characters\\GVTJ_82_G--320.png\n",
      "contour saved: cropped_characters\\GVTJ_82_V--321.png\n",
      "contour saved: cropped_characters\\GVTJ_82_T--322.png\n",
      "contour saved: cropped_characters\\GVTJ_82_J--323.png\n",
      "contour saved: cropped_characters\\GX8H_83_G--324.png\n",
      "contour saved: cropped_characters\\GX8H_83_X--325.png\n",
      "contour saved: cropped_characters\\GX8H_83_8--326.png\n",
      "contour saved: cropped_characters\\GX8H_83_H--327.png\n",
      "contour saved: cropped_characters\\H97Z_84_H--328.png\n",
      "contour saved: cropped_characters\\H97Z_84_9--329.png\n",
      "contour saved: cropped_characters\\H97Z_84_7--330.png\n",
      "contour saved: cropped_characters\\H97Z_84_Z--331.png\n",
      "contour saved: cropped_characters\\I55S_85_I--332.png\n",
      "contour saved: cropped_characters\\I55S_85_5--333.png\n",
      "contour saved: cropped_characters\\I55S_85_5--334.png\n",
      "contour saved: cropped_characters\\I55S_85_S--335.png\n",
      "contour saved: cropped_characters\\I8SO_86_I--336.png\n",
      "contour saved: cropped_characters\\I8SO_86_8--337.png\n",
      "contour saved: cropped_characters\\I8SO_86_S--338.png\n",
      "contour saved: cropped_characters\\I8SO_86_O--339.png\n",
      "contour saved: cropped_characters\\I91C_87_I--340.png\n",
      "contour saved: cropped_characters\\I91C_87_9--341.png\n",
      "contour saved: cropped_characters\\I91C_87_1--342.png\n",
      "contour saved: cropped_characters\\I91C_87_C--343.png\n",
      "contour saved: cropped_characters\\IFPY_88_I--344.png\n",
      "contour saved: cropped_characters\\IFPY_88_F--345.png\n",
      "contour saved: cropped_characters\\IFPY_88_P--346.png\n",
      "contour saved: cropped_characters\\IFPY_88_Y--347.png\n",
      "contour saved: cropped_characters\\IRYH_89_I--348.png\n",
      "contour saved: cropped_characters\\IRYH_89_R--349.png\n",
      "contour saved: cropped_characters\\IRYH_89_Y--350.png\n",
      "contour saved: cropped_characters\\IRYH_89_H--351.png\n",
      "contour saved: cropped_characters\\ITK9_90_I--352.png\n",
      "contour saved: cropped_characters\\ITK9_90_T--353.png\n",
      "contour saved: cropped_characters\\ITK9_90_K--354.png\n",
      "contour saved: cropped_characters\\ITK9_90_9--355.png\n",
      "contour saved: cropped_characters\\IVWJ_91_I--356.png\n",
      "contour saved: cropped_characters\\IVWJ_91_V--357.png\n",
      "contour saved: cropped_characters\\IVWJ_91_W--358.png\n",
      "contour saved: cropped_characters\\IVWJ_91_J--359.png\n",
      "contour saved: cropped_characters\\J1G8_92_J--360.png\n",
      "contour saved: cropped_characters\\J1G8_92_1--361.png\n",
      "contour saved: cropped_characters\\J1G8_92_G--362.png\n",
      "contour saved: cropped_characters\\J1G8_92_8--363.png\n",
      "contour saved: cropped_characters\\J8FN_93_J--364.png\n",
      "contour saved: cropped_characters\\J8FN_93_8--365.png\n",
      "contour saved: cropped_characters\\J8FN_93_F--366.png\n",
      "contour saved: cropped_characters\\J8FN_93_N--367.png\n",
      "contour saved: cropped_characters\\JG9G_94_J--368.png\n",
      "contour saved: cropped_characters\\JG9G_94_G--369.png\n",
      "contour saved: cropped_characters\\JG9G_94_9--370.png\n",
      "contour saved: cropped_characters\\JG9G_94_G--371.png\n",
      "contour saved: cropped_characters\\JGTX_95_J--372.png\n",
      "contour saved: cropped_characters\\JGTX_95_G--373.png\n",
      "contour saved: cropped_characters\\JGTX_95_T--374.png\n",
      "contour saved: cropped_characters\\JGTX_95_X--375.png\n",
      "contour saved: cropped_characters\\JM4B_96_J--376.png\n",
      "contour saved: cropped_characters\\JM4B_96_M--377.png\n",
      "contour saved: cropped_characters\\JM4B_96_4--378.png\n",
      "contour saved: cropped_characters\\JM4B_96_B--379.png\n",
      "contour saved: cropped_characters\\JM5B_97_J--380.png\n",
      "contour saved: cropped_characters\\JM5B_97_M--381.png\n",
      "contour saved: cropped_characters\\JM5B_97_5--382.png\n",
      "contour saved: cropped_characters\\JM5B_97_B--383.png\n",
      "contour saved: cropped_characters\\JO9J_98_J--384.png\n",
      "contour saved: cropped_characters\\JO9J_98_O--385.png\n",
      "contour saved: cropped_characters\\JO9J_98_9--386.png\n",
      "contour saved: cropped_characters\\JO9J_98_J--387.png\n",
      "contour saved: cropped_characters\\JR76_99_J--388.png\n",
      "contour saved: cropped_characters\\JR76_99_R--389.png\n",
      "contour saved: cropped_characters\\JR76_99_7--390.png\n",
      "contour saved: cropped_characters\\JR76_99_6--391.png\n",
      "contour saved: cropped_characters\\JT2S_100_J--392.png\n",
      "contour saved: cropped_characters\\JT2S_100_T--393.png\n",
      "contour saved: cropped_characters\\JT2S_100_2--394.png\n",
      "contour saved: cropped_characters\\JT2S_100_S--395.png\n",
      "contour saved: cropped_characters\\K4DQ_101_K--396.png\n",
      "contour saved: cropped_characters\\K4DQ_101_4--397.png\n",
      "contour saved: cropped_characters\\K4DQ_101_D--398.png\n",
      "contour saved: cropped_characters\\K4DQ_101_Q--399.png\n",
      "contour saved: cropped_characters\\KAKE_102_K--400.png\n",
      "contour saved: cropped_characters\\KAKE_102_A--401.png\n",
      "contour saved: cropped_characters\\KAKE_102_K--402.png\n",
      "contour saved: cropped_characters\\KAKE_102_E--403.png\n",
      "contour saved: cropped_characters\\KLZR_103_K--404.png\n",
      "contour saved: cropped_characters\\KLZR_103_L--405.png\n",
      "contour saved: cropped_characters\\KLZR_103_Z--406.png\n",
      "contour saved: cropped_characters\\KLZR_103_R--407.png\n",
      "contour saved: cropped_characters\\KVBK_104_K--408.png\n",
      "contour saved: cropped_characters\\KVBK_104_V--409.png\n",
      "contour saved: cropped_characters\\KVBK_104_B--410.png\n",
      "contour saved: cropped_characters\\KVBK_104_K--411.png\n",
      "contour saved: cropped_characters\\KWW7_105_K--412.png\n",
      "contour saved: cropped_characters\\KWW7_105_W--413.png\n",
      "contour saved: cropped_characters\\KWW7_105_W--414.png\n",
      "contour saved: cropped_characters\\KWW7_105_7--415.png\n",
      "contour saved: cropped_characters\\LDI1_106_L--416.png\n",
      "contour saved: cropped_characters\\LDI1_106_D--417.png\n",
      "contour saved: cropped_characters\\LDI1_106_I--418.png\n",
      "contour saved: cropped_characters\\LDI1_106_1--419.png\n",
      "contour saved: cropped_characters\\LEJV_107_L--420.png\n",
      "contour saved: cropped_characters\\LEJV_107_E--421.png\n",
      "contour saved: cropped_characters\\LEJV_107_J--422.png\n",
      "contour saved: cropped_characters\\LEJV_107_V--423.png\n",
      "contour saved: cropped_characters\\LPW4_108_L--424.png\n",
      "contour saved: cropped_characters\\LPW4_108_P--425.png\n",
      "contour saved: cropped_characters\\LPW4_108_W--426.png\n",
      "contour saved: cropped_characters\\LPW4_108_4--427.png\n",
      "contour saved: cropped_characters\\M1NL_109_M--428.png\n",
      "contour saved: cropped_characters\\M1NL_109_1--429.png\n",
      "contour saved: cropped_characters\\M1NL_109_N--430.png\n",
      "contour saved: cropped_characters\\M1NL_109_L--431.png\n",
      "contour saved: cropped_characters\\M22A_110_M--432.png\n",
      "contour saved: cropped_characters\\M22A_110_2--433.png\n",
      "contour saved: cropped_characters\\M22A_110_2--434.png\n",
      "contour saved: cropped_characters\\M22A_110_A--435.png\n",
      "contour saved: cropped_characters\\M42L_111_M--436.png\n",
      "contour saved: cropped_characters\\M42L_111_4--437.png\n",
      "contour saved: cropped_characters\\M42L_111_2--438.png\n",
      "contour saved: cropped_characters\\M42L_111_L--439.png\n",
      "contour saved: cropped_characters\\MME2_112_M--440.png\n",
      "contour saved: cropped_characters\\MME2_112_M--441.png\n",
      "contour saved: cropped_characters\\MME2_112_E--442.png\n",
      "contour saved: cropped_characters\\MME2_112_2--443.png\n",
      "contour saved: cropped_characters\\N5VN_113_N--444.png\n",
      "contour saved: cropped_characters\\N5VN_113_5--445.png\n",
      "contour saved: cropped_characters\\N5VN_113_V--446.png\n",
      "contour saved: cropped_characters\\N5VN_113_N--447.png\n",
      "contour saved: cropped_characters\\NEAJ_114_N--448.png\n",
      "contour saved: cropped_characters\\NEAJ_114_E--449.png\n",
      "contour saved: cropped_characters\\NEAJ_114_A--450.png\n",
      "contour saved: cropped_characters\\NEAJ_114_J--451.png\n",
      "contour saved: cropped_characters\\NTR2_115_N--452.png\n",
      "contour saved: cropped_characters\\NTR2_115_T--453.png\n",
      "contour saved: cropped_characters\\NTR2_115_R--454.png\n",
      "contour saved: cropped_characters\\NTR2_115_2--455.png\n",
      "contour saved: cropped_characters\\O6OA_116_O--456.png\n",
      "contour saved: cropped_characters\\O6OA_116_6--457.png\n",
      "contour saved: cropped_characters\\O6OA_116_O--458.png\n",
      "contour saved: cropped_characters\\O6OA_116_A--459.png\n",
      "contour saved: cropped_characters\\OCMM_117_O--460.png\n",
      "contour saved: cropped_characters\\OCMM_117_C--461.png\n",
      "contour saved: cropped_characters\\OCMM_117_M--462.png\n",
      "contour saved: cropped_characters\\OCMM_117_M--463.png\n",
      "contour saved: cropped_characters\\ODQ6_118_O--464.png\n",
      "contour saved: cropped_characters\\ODQ6_118_D--465.png\n",
      "contour saved: cropped_characters\\ODQ6_118_Q--466.png\n",
      "contour saved: cropped_characters\\ODQ6_118_6--467.png\n",
      "contour saved: cropped_characters\\OPVS_119_O--468.png\n",
      "contour saved: cropped_characters\\OPVS_119_P--469.png\n",
      "contour saved: cropped_characters\\OPVS_119_V--470.png\n",
      "contour saved: cropped_characters\\OPVS_119_S--471.png\n",
      "contour saved: cropped_characters\\OQCZ_120_O--472.png\n",
      "contour saved: cropped_characters\\OQCZ_120_Q--473.png\n",
      "contour saved: cropped_characters\\OQCZ_120_C--474.png\n",
      "contour saved: cropped_characters\\OQCZ_120_Z--475.png\n",
      "contour saved: cropped_characters\\P1H7_121_P--476.png\n",
      "contour saved: cropped_characters\\P1H7_121_1--477.png\n",
      "contour saved: cropped_characters\\P1H7_121_H--478.png\n",
      "contour saved: cropped_characters\\P1H7_121_7--479.png\n",
      "contour saved: cropped_characters\\P76J_122_P--480.png\n",
      "contour saved: cropped_characters\\P76J_122_7--481.png\n",
      "contour saved: cropped_characters\\P76J_122_6--482.png\n",
      "contour saved: cropped_characters\\P76J_122_J--483.png\n",
      "contour saved: cropped_characters\\PATU_123_P--484.png\n",
      "contour saved: cropped_characters\\PATU_123_A--485.png\n",
      "contour saved: cropped_characters\\PATU_123_T--486.png\n",
      "contour saved: cropped_characters\\PATU_123_U--487.png\n",
      "contour saved: cropped_characters\\PLGL_124_P--488.png\n",
      "contour saved: cropped_characters\\PLGL_124_L--489.png\n",
      "contour saved: cropped_characters\\PLGL_124_G--490.png\n",
      "contour saved: cropped_characters\\PLGL_124_L--491.png\n",
      "contour saved: cropped_characters\\QAA4_125_Q--492.png\n",
      "contour saved: cropped_characters\\QAA4_125_A--493.png\n",
      "contour saved: cropped_characters\\QAA4_125_A--494.png\n",
      "contour saved: cropped_characters\\QAA4_125_4--495.png\n",
      "contour saved: cropped_characters\\QECC_126_Q--496.png\n",
      "contour saved: cropped_characters\\QECC_126_E--497.png\n",
      "contour saved: cropped_characters\\QECC_126_C--498.png\n",
      "contour saved: cropped_characters\\QECC_126_C--499.png\n",
      "contour saved: cropped_characters\\QTX8_127_Q--500.png\n",
      "contour saved: cropped_characters\\QTX8_127_T--501.png\n",
      "contour saved: cropped_characters\\QTX8_127_X--502.png\n",
      "contour saved: cropped_characters\\QTX8_127_8--503.png\n",
      "contour saved: cropped_characters\\R7DD_128_R--504.png\n",
      "contour saved: cropped_characters\\R7DD_128_7--505.png\n",
      "contour saved: cropped_characters\\R7DD_128_D--506.png\n",
      "contour saved: cropped_characters\\R7DD_128_D--507.png\n",
      "contour saved: cropped_characters\\RBSY_129_R--508.png\n",
      "contour saved: cropped_characters\\RBSY_129_B--509.png\n",
      "contour saved: cropped_characters\\RBSY_129_S--510.png\n",
      "contour saved: cropped_characters\\RBSY_129_Y--511.png\n",
      "contour saved: cropped_characters\\RGRJ_130_R--512.png\n",
      "contour saved: cropped_characters\\RGRJ_130_G--513.png\n",
      "contour saved: cropped_characters\\RGRJ_130_R--514.png\n",
      "contour saved: cropped_characters\\RGRJ_130_J--515.png\n",
      "contour saved: cropped_characters\\RLKI_131_R--516.png\n",
      "contour saved: cropped_characters\\RLKI_131_L--517.png\n",
      "contour saved: cropped_characters\\RLKI_131_K--518.png\n",
      "contour saved: cropped_characters\\RLKI_131_I--519.png\n",
      "contour saved: cropped_characters\\RVJI_132_R--520.png\n",
      "contour saved: cropped_characters\\RVJI_132_V--521.png\n",
      "contour saved: cropped_characters\\RVJI_132_J--522.png\n",
      "contour saved: cropped_characters\\RVJI_132_I--523.png\n",
      "contour saved: cropped_characters\\S1PS_133_S--524.png\n",
      "contour saved: cropped_characters\\S1PS_133_1--525.png\n",
      "contour saved: cropped_characters\\S1PS_133_P--526.png\n",
      "contour saved: cropped_characters\\S1PS_133_S--527.png\n",
      "contour saved: cropped_characters\\S4RI_134_S--528.png\n",
      "contour saved: cropped_characters\\S4RI_134_4--529.png\n",
      "contour saved: cropped_characters\\S4RI_134_R--530.png\n",
      "contour saved: cropped_characters\\S4RI_134_I--531.png\n",
      "contour saved: cropped_characters\\SDYP_135_S--532.png\n",
      "contour saved: cropped_characters\\SDYP_135_D--533.png\n",
      "contour saved: cropped_characters\\SDYP_135_Y--534.png\n",
      "contour saved: cropped_characters\\SDYP_135_P--535.png\n",
      "contour saved: cropped_characters\\SKIW_136_S--536.png\n",
      "contour saved: cropped_characters\\SKIW_136_K--537.png\n",
      "contour saved: cropped_characters\\SKIW_136_I--538.png\n",
      "contour saved: cropped_characters\\SKIW_136_W--539.png\n",
      "contour saved: cropped_characters\\SR6Y_137_S--540.png\n",
      "contour saved: cropped_characters\\SR6Y_137_R--541.png\n",
      "contour saved: cropped_characters\\SR6Y_137_6--542.png\n",
      "contour saved: cropped_characters\\SR6Y_137_Y--543.png\n",
      "contour saved: cropped_characters\\SSED_138_S--544.png\n",
      "contour saved: cropped_characters\\SSED_138_S--545.png\n",
      "contour saved: cropped_characters\\SSED_138_E--546.png\n",
      "contour saved: cropped_characters\\SSED_138_D--547.png\n",
      "contour saved: cropped_characters\\SVG4_139_S--548.png\n",
      "contour saved: cropped_characters\\SVG4_139_V--549.png\n",
      "contour saved: cropped_characters\\SVG4_139_G--550.png\n",
      "contour saved: cropped_characters\\SVG4_139_4--551.png\n",
      "contour saved: cropped_characters\\T2CJ_140_T--552.png\n",
      "contour saved: cropped_characters\\T2CJ_140_2--553.png\n",
      "contour saved: cropped_characters\\T2CJ_140_C--554.png\n",
      "contour saved: cropped_characters\\T2CJ_140_J--555.png\n",
      "contour saved: cropped_characters\\T6X9_141_T--556.png\n",
      "contour saved: cropped_characters\\T6X9_141_6--557.png\n",
      "contour saved: cropped_characters\\T6X9_141_X--558.png\n",
      "contour saved: cropped_characters\\T6X9_141_9--559.png\n",
      "contour saved: cropped_characters\\TFWI_142_T--560.png\n",
      "contour saved: cropped_characters\\TFWI_142_F--561.png\n",
      "contour saved: cropped_characters\\TFWI_142_W--562.png\n",
      "contour saved: cropped_characters\\TFWI_142_I--563.png\n",
      "contour saved: cropped_characters\\TI1I_143_T--564.png\n",
      "contour saved: cropped_characters\\TI1I_143_I--565.png\n",
      "contour saved: cropped_characters\\TI1I_143_1--566.png\n",
      "contour saved: cropped_characters\\TI1I_143_I--567.png\n",
      "contour saved: cropped_characters\\TPPT_144_T--568.png\n",
      "contour saved: cropped_characters\\TPPT_144_P--569.png\n",
      "contour saved: cropped_characters\\TPPT_144_P--570.png\n",
      "contour saved: cropped_characters\\TPPT_144_T--571.png\n",
      "contour saved: cropped_characters\\TW7K_145_T--572.png\n",
      "contour saved: cropped_characters\\TW7K_145_W--573.png\n",
      "contour saved: cropped_characters\\TW7K_145_7--574.png\n",
      "contour saved: cropped_characters\\TW7K_145_K--575.png\n",
      "contour saved: cropped_characters\\TZ2R_146_T--576.png\n",
      "contour saved: cropped_characters\\TZ2R_146_Z--577.png\n",
      "contour saved: cropped_characters\\TZ2R_146_2--578.png\n",
      "contour saved: cropped_characters\\TZ2R_146_R--579.png\n",
      "contour saved: cropped_characters\\U4US_147_U--580.png\n",
      "contour saved: cropped_characters\\U4US_147_4--581.png\n",
      "contour saved: cropped_characters\\U4US_147_U--582.png\n",
      "contour saved: cropped_characters\\U4US_147_S--583.png\n",
      "contour saved: cropped_characters\\U949_148_U--584.png\n",
      "contour saved: cropped_characters\\U949_148_9--585.png\n",
      "contour saved: cropped_characters\\U949_148_4--586.png\n",
      "contour saved: cropped_characters\\U949_148_9--587.png\n",
      "contour saved: cropped_characters\\UAJC_149_U--588.png\n",
      "contour saved: cropped_characters\\UAJC_149_A--589.png\n",
      "contour saved: cropped_characters\\UAJC_149_J--590.png\n",
      "contour saved: cropped_characters\\UAJC_149_C--591.png\n",
      "contour saved: cropped_characters\\UBNA_150_U--592.png\n",
      "contour saved: cropped_characters\\UBNA_150_B--593.png\n",
      "contour saved: cropped_characters\\UBNA_150_N--594.png\n",
      "contour saved: cropped_characters\\UBNA_150_A--595.png\n",
      "contour saved: cropped_characters\\UCZH_151_U--596.png\n",
      "contour saved: cropped_characters\\UCZH_151_C--597.png\n",
      "contour saved: cropped_characters\\UCZH_151_Z--598.png\n",
      "contour saved: cropped_characters\\UCZH_151_H--599.png\n",
      "contour saved: cropped_characters\\UD68_152_U--600.png\n",
      "contour saved: cropped_characters\\UD68_152_D--601.png\n",
      "contour saved: cropped_characters\\UD68_152_6--602.png\n",
      "contour saved: cropped_characters\\UD68_152_8--603.png\n",
      "contour saved: cropped_characters\\UL8K_153_U--604.png\n",
      "contour saved: cropped_characters\\UL8K_153_L--605.png\n",
      "contour saved: cropped_characters\\UL8K_153_8--606.png\n",
      "contour saved: cropped_characters\\UL8K_153_K--607.png\n",
      "contour saved: cropped_characters\\UO12_154_U--608.png\n",
      "contour saved: cropped_characters\\UO12_154_O--609.png\n",
      "contour saved: cropped_characters\\UO12_154_1--610.png\n",
      "contour saved: cropped_characters\\UO12_154_2--611.png\n",
      "contour saved: cropped_characters\\UOYA_155_U--612.png\n",
      "contour saved: cropped_characters\\UOYA_155_O--613.png\n",
      "contour saved: cropped_characters\\UOYA_155_Y--614.png\n",
      "contour saved: cropped_characters\\UOYA_155_A--615.png\n",
      "contour saved: cropped_characters\\UPVK_156_U--616.png\n",
      "contour saved: cropped_characters\\UPVK_156_P--617.png\n",
      "contour saved: cropped_characters\\UPVK_156_V--618.png\n",
      "contour saved: cropped_characters\\UPVK_156_K--619.png\n",
      "contour saved: cropped_characters\\UYYQ_157_U--620.png\n",
      "contour saved: cropped_characters\\UYYQ_157_Y--621.png\n",
      "contour saved: cropped_characters\\UYYQ_157_Y--622.png\n",
      "contour saved: cropped_characters\\UYYQ_157_Q--623.png\n",
      "contour saved: cropped_characters\\V4CR_158_V--624.png\n",
      "contour saved: cropped_characters\\V4CR_158_4--625.png\n",
      "contour saved: cropped_characters\\V4CR_158_C--626.png\n",
      "contour saved: cropped_characters\\V4CR_158_R--627.png\n",
      "contour saved: cropped_characters\\V4XF_159_V--628.png\n",
      "contour saved: cropped_characters\\V4XF_159_4--629.png\n",
      "contour saved: cropped_characters\\V4XF_159_X--630.png\n",
      "contour saved: cropped_characters\\V4XF_159_F--631.png\n",
      "contour saved: cropped_characters\\V9OK_160_V--632.png\n",
      "contour saved: cropped_characters\\V9OK_160_9--633.png\n",
      "contour saved: cropped_characters\\V9OK_160_O--634.png\n",
      "contour saved: cropped_characters\\V9OK_160_K--635.png\n",
      "contour saved: cropped_characters\\VE11_161_V--636.png\n",
      "contour saved: cropped_characters\\VE11_161_E--637.png\n",
      "contour saved: cropped_characters\\VE11_161_1--638.png\n",
      "contour saved: cropped_characters\\VE11_161_1--639.png\n",
      "contour saved: cropped_characters\\VLDR_162_V--640.png\n",
      "contour saved: cropped_characters\\VLDR_162_L--641.png\n",
      "contour saved: cropped_characters\\VLDR_162_D--642.png\n",
      "contour saved: cropped_characters\\VLDR_162_R--643.png\n",
      "contour saved: cropped_characters\\VN8Y_163_V--644.png\n",
      "contour saved: cropped_characters\\VN8Y_163_N--645.png\n",
      "contour saved: cropped_characters\\VN8Y_163_8--646.png\n",
      "contour saved: cropped_characters\\VN8Y_163_Y--647.png\n",
      "contour saved: cropped_characters\\W5JU_164_W--648.png\n",
      "contour saved: cropped_characters\\W5JU_164_5--649.png\n",
      "contour saved: cropped_characters\\W5JU_164_J--650.png\n",
      "contour saved: cropped_characters\\W5JU_164_U--651.png\n",
      "contour saved: cropped_characters\\WBS9_165_W--652.png\n",
      "contour saved: cropped_characters\\WBS9_165_B--653.png\n",
      "contour saved: cropped_characters\\WBS9_165_S--654.png\n",
      "contour saved: cropped_characters\\WBS9_165_9--655.png\n",
      "contour saved: cropped_characters\\WDIC_166_W--656.png\n",
      "contour saved: cropped_characters\\WDIC_166_D--657.png\n",
      "contour saved: cropped_characters\\WDIC_166_I--658.png\n",
      "contour saved: cropped_characters\\WDIC_166_C--659.png\n",
      "contour saved: cropped_characters\\WF4B_167_W--660.png\n",
      "contour saved: cropped_characters\\WF4B_167_F--661.png\n",
      "contour saved: cropped_characters\\WF4B_167_4--662.png\n",
      "contour saved: cropped_characters\\WF4B_167_B--663.png\n",
      "contour saved: cropped_characters\\WJL6_168_W--664.png\n",
      "contour saved: cropped_characters\\WJL6_168_J--665.png\n",
      "contour saved: cropped_characters\\WJL6_168_L--666.png\n",
      "contour saved: cropped_characters\\WJL6_168_6--667.png\n",
      "contour saved: cropped_characters\\WJTT_169_W--668.png\n",
      "contour saved: cropped_characters\\WJTT_169_J--669.png\n",
      "contour saved: cropped_characters\\WJTT_169_T--670.png\n",
      "contour saved: cropped_characters\\WJTT_169_T--671.png\n",
      "contour saved: cropped_characters\\X155_170_X--672.png\n",
      "contour saved: cropped_characters\\X155_170_1--673.png\n",
      "contour saved: cropped_characters\\X155_170_5--674.png\n",
      "contour saved: cropped_characters\\X155_170_5--675.png\n",
      "contour saved: cropped_characters\\X44V_171_X--676.png\n",
      "contour saved: cropped_characters\\X44V_171_4--677.png\n",
      "contour saved: cropped_characters\\X44V_171_4--678.png\n",
      "contour saved: cropped_characters\\X44V_171_V--679.png\n",
      "contour saved: cropped_characters\\X6AD_172_X--680.png\n",
      "contour saved: cropped_characters\\X6AD_172_6--681.png\n",
      "contour saved: cropped_characters\\X6AD_172_A--682.png\n",
      "contour saved: cropped_characters\\X6AD_172_D--683.png\n",
      "contour saved: cropped_characters\\X7X7_173_X--684.png\n",
      "contour saved: cropped_characters\\X7X7_173_7--685.png\n",
      "contour saved: cropped_characters\\X7X7_173_X--686.png\n",
      "contour saved: cropped_characters\\X7X7_173_7--687.png\n",
      "contour saved: cropped_characters\\X82B_174_X--688.png\n",
      "contour saved: cropped_characters\\X82B_174_8--689.png\n",
      "contour saved: cropped_characters\\X82B_174_2--690.png\n",
      "contour saved: cropped_characters\\X82B_174_B--691.png\n",
      "contour saved: cropped_characters\\XF8J_175_X--692.png\n",
      "contour saved: cropped_characters\\XF8J_175_F--693.png\n",
      "contour saved: cropped_characters\\XF8J_175_8--694.png\n",
      "contour saved: cropped_characters\\XF8J_175_J--695.png\n",
      "contour saved: cropped_characters\\XFVE_176_X--696.png\n",
      "contour saved: cropped_characters\\XFVE_176_F--697.png\n",
      "contour saved: cropped_characters\\XFVE_176_V--698.png\n",
      "contour saved: cropped_characters\\XFVE_176_E--699.png\n",
      "contour saved: cropped_characters\\XO9V_177_X--700.png\n",
      "contour saved: cropped_characters\\XO9V_177_O--701.png\n",
      "contour saved: cropped_characters\\XO9V_177_9--702.png\n",
      "contour saved: cropped_characters\\XO9V_177_V--703.png\n",
      "contour saved: cropped_characters\\XP72_178_X--704.png\n",
      "contour saved: cropped_characters\\XP72_178_P--705.png\n",
      "contour saved: cropped_characters\\XP72_178_7--706.png\n",
      "contour saved: cropped_characters\\XP72_178_2--707.png\n",
      "contour saved: cropped_characters\\Y5P4_179_Y--708.png\n",
      "contour saved: cropped_characters\\Y5P4_179_5--709.png\n",
      "contour saved: cropped_characters\\Y5P4_179_P--710.png\n",
      "contour saved: cropped_characters\\Y5P4_179_4--711.png\n",
      "contour saved: cropped_characters\\Y6C9_180_Y--712.png\n",
      "contour saved: cropped_characters\\Y6C9_180_6--713.png\n",
      "contour saved: cropped_characters\\Y6C9_180_C--714.png\n",
      "contour saved: cropped_characters\\Y6C9_180_9--715.png\n",
      "contour saved: cropped_characters\\YFSD_181_Y--716.png\n",
      "contour saved: cropped_characters\\YFSD_181_F--717.png\n",
      "contour saved: cropped_characters\\YFSD_181_S--718.png\n",
      "contour saved: cropped_characters\\YFSD_181_D--719.png\n",
      "contour saved: cropped_characters\\YH2E_182_Y--720.png\n",
      "contour saved: cropped_characters\\YH2E_182_H--721.png\n",
      "contour saved: cropped_characters\\YH2E_182_2--722.png\n",
      "contour saved: cropped_characters\\YH2E_182_E--723.png\n",
      "contour saved: cropped_characters\\YOOX_183_Y--724.png\n",
      "contour saved: cropped_characters\\YOOX_183_O--725.png\n",
      "contour saved: cropped_characters\\YOOX_183_O--726.png\n",
      "contour saved: cropped_characters\\YOOX_183_X--727.png\n",
      "contour saved: cropped_characters\\Z3JX_184_Z--728.png\n",
      "contour saved: cropped_characters\\Z3JX_184_3--729.png\n",
      "contour saved: cropped_characters\\Z3JX_184_J--730.png\n",
      "contour saved: cropped_characters\\Z3JX_184_X--731.png\n",
      "contour saved: cropped_characters\\Z8A1_185_Z--732.png\n",
      "contour saved: cropped_characters\\Z8A1_185_8--733.png\n",
      "contour saved: cropped_characters\\Z8A1_185_A--734.png\n",
      "contour saved: cropped_characters\\Z8A1_185_1--735.png\n",
      "contour saved: cropped_characters\\ZFHA_186_Z--736.png\n",
      "contour saved: cropped_characters\\ZFHA_186_F--737.png\n",
      "contour saved: cropped_characters\\ZFHA_186_H--738.png\n",
      "contour saved: cropped_characters\\ZFHA_186_A--739.png\n",
      "contour saved: cropped_characters\\ZKQD_187_Z--740.png\n",
      "contour saved: cropped_characters\\ZKQD_187_K--741.png\n",
      "contour saved: cropped_characters\\ZKQD_187_Q--742.png\n",
      "contour saved: cropped_characters\\ZKQD_187_D--743.png\n",
      "contour saved: cropped_characters\\ZVOF_188_Z--744.png\n",
      "contour saved: cropped_characters\\ZVOF_188_V--745.png\n",
      "contour saved: cropped_characters\\ZVOF_188_O--746.png\n",
      "contour saved: cropped_characters\\ZVOF_188_F--747.png\n"
     ]
    }
   ],
   "source": [
    "# Folder path containing the images\n",
    "folder_path = 'denoised_images' # change folders for testing\n",
    "\n",
    "# Output directory for saved contour images\n",
    "output_directory = 'cropped_characters' # change folders for testing\n",
    "\n",
    "# Get list of all files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "image_id = 0\n",
    "\n",
    "# Iterate through the first 10 images in the folder\n",
    "for i, filename in enumerate(file_list):\n",
    "    # Check if the file is an image (you can add more image extensions if needed)\n",
    "    if filename.lower().endswith(('.png')):\n",
    "        # Construct the full path to the image\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Call the save_contours_as_images function\n",
    "        save_contours_as_images(image_path, output_directory, image_id)\n",
    "\n",
    "        image_id += 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Class for Cropped Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] # 35 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CroppedCharacterDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(root_dir, img) for img in os.listdir(root_dir) if img.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('L')  # Convert image to grayscale\n",
    "        \n",
    "        # Extract label and image id from file name\n",
    "        filename = os.path.splitext(os.path.basename(img_path))[0]\n",
    "\n",
    "        # format: {ParentImageName}_{image_num}_{char_label}--{id}.png\n",
    "        parts = filename.split('_')\n",
    "        label, image_id = parts[-1].split('--')  # Split last part\n",
    "        label = label.strip()  # Remove any leading/trailing whitespace\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root directory where images are stored\n",
    "root_dir = \"cropped_characters\"\n",
    "\n",
    "# Define transformations if needed\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create dataset instance\n",
    "cropped_chars_dataset = CroppedCharacterDataset(root_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sizes of each split\n",
    "print(len(cropped_chars_dataset)) # total number of images in dataset\n",
    "train_size = int(1 * len(cropped_chars_dataset))\n",
    "# dev_size = int(0.2 * len(cropped_chars_dataset))\n",
    "# test_size = len(cropped_chars_dataset) - train_size - dev_size\n",
    "\n",
    "# Split dataset into train, validation, and test sets\n",
    "# train_data, dev_data, test_data = random_split(cropped_chars_dataset, [train_size, dev_size, test_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cropped_chars_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create data loaders\n",
    "# train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "# dev_loader = DataLoader(dev_data, batch_size=32, shuffle=False)\n",
    "# test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_loader.dataset)} examples into {len(train_loader)} batches\")\n",
    "# print(f\"Test: {len(test_loader.dataset)} examples into {len(test_loader)} batches\")\n",
    "# print(f\"Test: {len(dev_loader.dataset)} examples into {len(dev_loader)} batches\")\n",
    "\n",
    "all_classes = {character : 0 for character in classes}\n",
    "print(all_classes)\n",
    "# print(train_loader.dataset[0])\n",
    "for data in train_loader.dataset:\n",
    "    image, label = data\n",
    "    all_classes[label] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'bar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(all_classes\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Plotting the number of labels for each possible class. \u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbar\u001b[49m(labels, values)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKeys\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAmount of labels\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\luizf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\_api\\__init__.py:217\u001b[0m, in \u001b[0;36mcaching_module_getattr.<locals>.__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m props:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m props[name]\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance)\n\u001b[1;32m--> 217\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'bar'"
     ]
    }
   ],
   "source": [
    "# Extract keys and values from the dictionary\n",
    "labels = list(all_classes.keys())\n",
    "values = list(all_classes.values())\n",
    "\n",
    "# Plotting the number of labels for each possible class. \n",
    "plt.bar(labels, values)\n",
    "plt.xlabel('Keys')\n",
    "plt.ylabel('Amount of labels')\n",
    "plt.title('Image Label Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `train_model(model, dataloader, epochs)`\n",
    "\n",
    "This function should train the given model using the given data for the given number of epochs.\n",
    "\n",
    "**Arguments**\n",
    " * `model`: A PyTorch model.  You can assume here that it has already been moved to `device` (and you should assure that is the case when calling this function).\n",
    " * `dataloader`: A PyTorch DataLoader.\n",
    " * `epochs`: The number of full epochs to train.\n",
    "\n",
    "**Return value**\n",
    " * None.\n",
    "\n",
    "Use `torch.nn.CrossEntropyLoss()` for the loss function and `optim.Adam` with its default hyperparameters for the optimization algorithm.\n",
    "\n",
    "Use the `stats` object to print training statistics before, during, and after the training run (see above for instructions).\n",
    "\n",
    "Don't forget to move all tensors to `device` so they are placed on the GPU if one is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, epochs):\n",
    "    # Starting stats object\n",
    "    # stats.start()\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Loop over the dataset for the given number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        # Iterate over the batches of data\n",
    "        for inputs, labels in dataloader:\n",
    "            # Move inputs and labels to GPU if possible\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get outputs from forward propagation\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward propagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Perform a single optimization step (parameter update)\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes=35):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        # self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        # self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        # self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # self.dropout = nn.Dropout(0.25)\n",
    "        self.fc1 = nn.Linear(128 * 12 * 12, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 12 * 12)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, num_epochs, learning_rate):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            # images, labels = batch[0], batch[1]  # Unpack the batch tuple and move tensors to the configured device\n",
    "            images = images.to(device)\n",
    "            # Extract and convert labels to integers\n",
    "            label_strings = [label_tuple[0] for label_tuple in labels]\n",
    "            label_indices = [label_strings.index(label) for label in label_strings]\n",
    "            labels = torch.tensor(label_indices, dtype=torch.long).to(device)\n",
    "            \n",
    "            # zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward propagation\n",
    "            outputs = model(images)\n",
    "\n",
    "            # computer loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # backward prop\n",
    "            loss.backward()\n",
    "\n",
    "            # update\n",
    "            optimizer.step()\n",
    "            \n",
    "            # calculating stats\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = correct / total\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {100 * train_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Move the model to GPU if not already in there\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, num_epochs, learning_rate)\u001b[0m\n\u001b[0;32m     32\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# update\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# calculating stats\u001b[39;00m\n\u001b[0;32m     38\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\luizf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\luizf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\luizf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\luizf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\luizf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:439\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    437\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    441\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    443\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get ResNet18 model with randomized weights\n",
    "model = CNNModel()\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Move the model to GPU if not already in there\n",
    "train_model(model, train_loader, epochs, learning_rate)\n",
    "\n",
    "# Check accuacy without training\n",
    "# overall_accuracy, class_accuracy = test_model(model, test_loader)\n",
    "# print(f'Accuracy before training: {overall_accuracy}')\n",
    "# \n",
    "# # Train the model for 3 epochs\n",
    "# train_model(model, train_loader, 5)\n",
    "# \n",
    "# # Checki accuracy after training\n",
    "# accuracy_after_training, class_accuracies_after_training = test_model(model, test_loader)\n",
    "# print(f'Accuracy after training: {accuracy_after_training}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
